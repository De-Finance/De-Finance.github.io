---
layout: post
title:  "[AI] 6. Multiclass Classification & Overcome Fitting Problem"
subtitle: " Learn about Multiclassification Modelling, L2 Regularization & Dropout"
category: [AI]
cover-img: /img/aipic.jpg
thumbnail-img: ""
tags: [AI]
comments: false
use_math: true
---

## 미래연구소 16기 강의 기반 정리

[http://futurelab.creatorlink.net/](http://futurelab.creatorlink.net/)

<br />

## 딥러닝 강의 6주차 - Multiclass Classification & Solve Fitting Problem

<br />
<br />

## 0) Deep Learning 과정 정리

![MLprocess](https://user-images.githubusercontent.com/86182583/129992667-67e8dbf3-052b-4ed2-b702-044db024fe0e.PNG)
<br />
<br />
범주 비교
![ml3](https://user-images.githubusercontent.com/86182583/129992812-60a0ac73-7503-4af9-a99e-8ad353210cb0.PNG)
<br />
<br />
## 1) Multiclass Classification

앞서 지난 example 까지 우리는 일치/불일치인지 이분법적으로 분류하는 문제들을 다뤘다.

이번 문제는 MNIST 같이 10가지 종류의 숫자를 다루는 문제에 관해 다룰 것이다.

다만 Binary Classification과 같이 0과 1 사이에 경계를 지여서 나누기엔 문제가 많다

0인 경계를 약간 벗어낫다고 그것을 1이라고 정의할 수 있나?

숫자에는 순서가 있지만, 컴퓨터가 형태를 읽고 결과를 내는 데이터는 독립적이다.

그림이 5와 비슷하게 보인다고 해서 4나 6이다라고 할 수 없다.

또한 균일하게 분류도 불가능하다. 아무리 균등하게 10등분 한다고 해도

Sigmoid 함수 맨 끝에 위치한 0이나 9는 가능한 범위가 무한으로 발산하기 때문에 오류를 갖을 확률이 높다.

이를 해결하려면 어떻게 할까?

결과(Output)를 Binary 대신 고차원으로 한다.

![mnist_model2](https://user-images.githubusercontent.com/86182583/129867168-e78f8e85-a869-4149-8f87-10e080b11a0f.PNG)

<br />

해결책은 벡터화하여 결과값을 처리하는 것이다.

MNIST에서는 shape(10,) 벡터를 만들고 각 위치마다 숫자를 배정해준다.

$$ x_{0} = \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix}  x_{1} = \begin{pmatrix} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}  x_{9} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix} $$

데이터 간 순서는 없다. N차원 공간을 만들어 처리한다.

이렇게 만드는 것을 One-Hot Encoding이라고 한다. 따라서 가운데 Layer의 설정은 마음대로 조정할 수 있지만, Output형태는 미리 설정해놔야 한다.

<br />
예시) 28 * 28 pixel로 이뤄진 이미지는 결국 숫자 형태로 인식해야 하기 때문에 Output의 경우가 10가지로 설정해서 나오는 것이다.

예를 들어 2라는 이미지를 대입했을 때, 처음 학습을 했더니 ŷ = [ 0.01 0.05 **0.7** 0.1 ... 0.2 0.03 ] 이런 식으로 나온다

이를 Loss 함수를 통해 Gradient를 설정해줘서 방향을 학습하고, 명확하게 구분되는 y = [ 0 0 1 0 0 ... 0 ] 으로 근사한다.

이렇게 계산하는 원리는 각 숫자마다 여러개의 Binary 문제를 푸는 것과 같다

다만 여기서 문제가 있다. 만일 이런 식으로 나온다고 가정해보자

ŷ = [ 0확률 1확률 2확률 ... 9확률 ]

ŷ = [ 0.7 0.4 0.9 ... 0.5 ] 이런 결과가 나왔을 때, 0일 확률도 무려 0.7인데

이를 무시하고 Sigmoid를 통해 나머지 값들을 0으로 보내는 것이 정확한 결과를 도출할지 의심스럽다.

따라서 전체에 대한 Class에 대한 확률로 계산할 수 있도록 만들어줘야 한다. (Σŷ ≠ 1 → Σŷ = 1)

이는 Softmax 함수를 통해서 할 수 있다. Softmax는 Sigmoid를 일반화한 형태라고 볼 수 있다.

$$ softmax(z) = \frac{e^{z_0}}{e^{z_0}+e^{z_1}+ \dots + e^{z_9}} = \frac{e^{z_i}}{\sum\limits_{i = 1}^9 {e^{z_i}}}$$

각 i에 softmax 함수를 적용하고 더하면 1을 만족한다.

Loss function을 적용시킨 것도 형태는 유사하다 (Cross Entropy)

$$ L(y, ŷ) = -\sum\limits_{i = 1}^n {y_{i}log(ŷ_i)} = -log(ŷ_i)$$

이 값들도 다 더하면 1이 나오게 된다.

<br />
<br />

지금까지 배운 것으로

상대적 Train Set ↑ = Underfit

상대적으로 Validation Set Error ↑ = Overfit 이다.

Underfitting 된 경우에 해결책은 **모델 사이즈를 늘려 복잡도를 높이거나, Training을 더 시킨다.**

한편, overfitting 같은 경우에는 2가지 방식이 있는데, Regularization과 Dropout을 사용하는 것이다.

<br />

## 2) L2 Regularization

![1200px-Regularization](https://user-images.githubusercontent.com/86182583/130037415-2a56522e-b9cb-4c13-82ef-ba2c376d3449.png)

Overfitting이 되면 weight의 값들이 너무 커져 특정한 값들에 치우치게 되는데,

이를 완화하는 역할을 하는 것이 정규화이다.

Weight Decay라고도 불린다.

L1과 L2가 있는데, 보통 L2를 더 선호한다.

![reg12](https://user-images.githubusercontent.com/86182583/130037534-ec3ae6f3-0b4f-4533-95d2-255ea2258b3e.png)

|| W || = [ $$ w_{1} w_{2} \dots w_{n} $$ ]

Cost Function인 J(W,b) 는

=$$ min_{\rm W,b}J(W,b) $$ → $$ min_{\rm W,b}J(W,b) $$ {$$ J(W,b) + \frac{\lambda}{2}|w|_{2} $$}

이렇게 뒤에 붙어서 학습을 해 나간다

우리가 노려야 할 것은 Cost인 J(w,b) 가 적당히 낮아지고, ||w|| 가 적당히 높은 점으로 타협을 해야 한다.

<br />
<br />

## 3) Dropout

Overfitting 해결책으로 Regularization 보다 Dropout을 더 많이 사용한다.

![dropout1](https://user-images.githubusercontent.com/86182583/130006814-b0f8dfb6-993a-430e-a477-b7ab06483556.png)

매 학습마다 설정한 Drop rate 만큼 Random하게 노드를 지워서 학습한다.

따라서 서로 다른 노드들이 쉬였다가 다시 사용된다.

이렇게 하면 특정 노드의 weight가 너무 치우치게 되는 것을 방지할 수 있다.

### 구현

Dropout Layer가 따로 만들지만, 실질적인 Layer로 카운팅 하진 않는다.

Dropout Layer를 만들면, 바로 위의 Layer를 Dropout을 실행한다.

아래 코드를 보자

![dropout2](https://user-images.githubusercontent.com/86182583/130009590-b0075171-adb6-42b0-a35d-f4a284682b4d.png)

`model.add(Dropout(0.2))` 는 바로 위인 `model.add(Dense(21, 'relu'))` 의 노드들에 Dropout을 적용한다.

보통 0.2 ~ 0.5 값을 많이 사용하는데, 0.2면 20% 노드는 그 세션에는 쉰다는 뜻이다.

rate를 크게 할 수록 Dropout 효과가 크다.


<br />
<br />
