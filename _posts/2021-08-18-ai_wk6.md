---
layout: post
title:  "[AI] 6. Multiclass Classification & Overcome Fitting Problem"
subtitle: " Learn about Multiclassification Modelling, L2 Regularization & Dropout"
category: [AI]
cover-img: /img/aipic.jpg
thumbnail-img: ""
tags: [AI]
comments: false
use_math: true
---

## 미래연구소 16기 강의 기반 정리

[http://futurelab.creatorlink.net/](http://futurelab.creatorlink.net/)

<br />

## 딥러닝 강의 6주차 - Multiclass Classification & Solve Fitting Problem

<br />
<br />

## 0) Deep Learning 과정 정리

![MLprocess](https://user-images.githubusercontent.com/86182583/129992667-67e8dbf3-052b-4ed2-b702-044db024fe0e.PNG)
<br />
<br />
범주 비교
![ml3](https://user-images.githubusercontent.com/86182583/129992812-60a0ac73-7503-4af9-a99e-8ad353210cb0.PNG)
<br />
<br />
## 1) Multiclass Classification

앞서 지난 example 까지 우리는 일치/불일치인지 이분법적으로 분류하는 문제들을 다뤘다.

이번 문제는 MNIST 같이 10가지 종류의 숫자를 다루는 문제에 관해 다룰 것이다.

다만 Binary Classification과 같이 0과 1 사이에 경계를 지여서 나누기엔 문제가 많다

0인 경계를 약간 벗어낫다고 그것을 1이라고 정의할 수 있나?

숫자에는 순서가 있지만, 컴퓨터가 형태를 읽고 결과를 내는 데이터는 독립적이다.

그림이 5와 비슷하게 보인다고 해서 4나 6이다라고 할 수 없다.

또한 균일하게 분류도 불가능하다. 아무리 균등하게 10등분 한다고 해도

Sigmoid 함수 맨 끝에 위치한 0이나 9는 가능한 범위가 무한으로 발산하기 때문에 오류를 갖을 확률이 높다.

이를 해결하려면 어떻게 할까?

결과(Output)를 Binary 대신 고차원으로 한다.

![mnist_model2](https://user-images.githubusercontent.com/86182583/129867168-e78f8e85-a869-4149-8f87-10e080b11a0f.PNG)

<br />

해결책은 벡터화하여 결과값을 처리하는 것이다.

MNIST에서는 shape(10,) 벡터를 만들고 각 위치마다 숫자를 배정해준다.

$$ x_{0} = \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix}  x_{1} = \begin{pmatrix} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}  x_{9} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix} $$

데이터 간 순서는 없다. N차원 공간을 만들어 처리한다.

이렇게 만드는 것을 One-Hot Encoding이라고 한다. 따라서 가운데 Layer의 설정은 마음대로 조정할 수 있지만, Output형태는 미리 설정해놔야 한다.

<br />
예시) 28 * 28 pixel로 이뤄진 이미지는 결국 숫자 형태로 인식해야 하기 때문에 Output의 경우가 10가지로 설정해서 나오는 것이다.

예를 들어 2라는 이미지를 대입했을 때, 처음 학습을 했더니 ŷ = [ 0.01 0.05 **0.7** 0.1 ... 0.2 0.03 ] 이런 식으로 나온다

이를 Loss 함수를 통해 Gradient를 설정해줘서 방향을 학습하고, 명확하게 구분되는 y = [ 0 0 1 0 0 ... 0 ] 으로 근사한다.

이렇게 계산하는 원리는 각 숫자마다 여러개의 Binary 문제를 푸는 것과 같다

다만 여기서 문제가 있다. 만일 이런 식으로 나온다고 가정해보자

ŷ = [ 0확률 1확률 2확률 ... 9확률 ]

ŷ = [ 0.7 0.4 0.9 ... 0.5 ] 이런 결과가 나왔을 때, 0일 확률도 무려 0.7인데

이를 무시하고 Sigmoid를 통해 나머지 값들을 0으로 보내는 것이 정확한 결과를 도출할지 의심스럽다.

따라서 전체에 대한 Class에 대한 확률로 계산할 수 있도록 만들어줘야 한다. (Σŷ ≠ 1 → Σŷ = 1)

이는 Softmax 함수를 통해서 할 수 있다. Softmax는 Sigmoid를 일반화한 형태라고 볼 수 있다.

$$ softmax(z) = \frac{e^{z_0}}{e^{z_0}+e^{z_1}+ \dots + e^{z_9}} = \frac{e^{z_i}}{\sum\limits_{i = 1}^9 {e^{z_i}}}$$

각 i에 softmax 함수를 적용하고 더하면 1을 만족한다.

Loss function을 적용시킨 것도 형태는 유사하다 (Cross Entropy)

$$ L(y, ŷ) = -\sum\limits_{i = 1}^n {y_{i}log(ŷ_i)} = -log(ŷ_i)$$

이 값들도 다 더하면 1이 나오게 된다.

---
<br />

지금까지 배운 것으로

상대적 Train Set ↑ = Underfit

상대적으로 Validation Set Error ↑ = Overfit 이다.

Underfitting 된 경우에 해결책은 **모델 사이즈를 늘려 복잡도를 높이거나, Training을 더 시킨다.**

한편, overfitting 같은 경우에는 2가지 방식이 있는데, Regularization과 Dropout을 사용하는 것이다.

<br />

## 2) L2 Regularization

![1200px-Regularization](https://user-images.githubusercontent.com/86182583/130037415-2a56522e-b9cb-4c13-82ef-ba2c376d3449.png)

Overfitting이 되면 weight의 값들이 너무 커져 특정한 값들에 치우치게 되는데,

이를 완화하는 역할을 하는 것이 정규화이다.

Weight Decay라고도 불린다.

Bias나 Activation에 가할 수도 있지만 보통 weight 값에 가한다.

L1과 L2가 있는데, 보통 L2를 더 선호한다.

![reg12](https://user-images.githubusercontent.com/86182583/130037534-ec3ae6f3-0b4f-4533-95d2-255ea2258b3e.png)

|| W || = [ $$ w_{1} w_{2} \dots w_{n} $$ ]

Cost Function인 J(W,b) 는

=$$ min_{\rm W,b}J(W,b) $$ → $$ min_{\rm W,b}J(W,b) $$ {$$ J(W,b) + \frac{\lambda}{2}|w|_{2} $$}

이렇게 뒤에 붙어서 학습을 해 나간다.

우리가 노려야 할 것은 Cost인 J(w,b) 가 적당히 낮아지고, ||w|| 가 적당히 높은 점으로 타협을 해야 한다.

람다 값으로 Weight Decay의 합의점을 조정하면서 찾아야한다.

람다가 크면 cost보다 ||w|| 가 부각되며, 0에 근접하면 ||w|| 의 영향력은 사라지고 cost function을 줄일 방향을 생각해야한다.

![l2reg](https://user-images.githubusercontent.com/86182583/130101864-3361cbf2-032f-4ae8-9bad-f0c6a00a12a6.PNG)

<br />
<br />

## 3) Dropout

Overfitting 해결책으로 Regularization 보다 Dropout을 더 많이 사용한다.

![dropout1](https://user-images.githubusercontent.com/86182583/130006814-b0f8dfb6-993a-430e-a477-b7ab06483556.png)

매 학습마다 설정한 Drop rate 만큼 Random하게 노드를 지워서 학습한다.

따라서 서로 다른 노드들이 쉬였다가 다시 사용된다.

이렇게 하면 특정 노드의 weight가 너무 치우치게 되는 것을 방지할 수 있다.

### 구현

Dropout Layer가 따로 만들지만, 실질적인 Layer로 카운팅 하진 않는다.

Dropout Layer를 만들면, 바로 위의 Layer를 Dropout을 실행한다.

아래 코드를 보자

![dropout2](https://user-images.githubusercontent.com/86182583/130009590-b0075171-adb6-42b0-a35d-f4a284682b4d.png)

`model.add(Dropout(0.2))` 는 바로 위인 `model.add(Dense(21, 'relu'))` 의 노드들에 Dropout을 적용한다.

보통 0.2 ~ 0.5 값을 많이 사용하는데, 0.2면 20% 노드는 그 세션에는 쉰다는 뜻이다.

rate를 크게 할 수록 Dropout 효과가 크다.


<br />
<br />
