---
layout: post
title:  "[AI] 7. Scaler, Batch, Optimizer"
subtitle: " Learn about  Scaler, Batch, Optimizer"
category: [AI]
cover-img: /img/aipic.jpg
thumbnail-img: ""
tags: [AI]
comments: false
use_math: true
---
<br />

## 딥러닝 강의 7주차 -  Scaler, Batch, Optimizer

<br />

## 1) Scaler

소위 Feature 값들을 0~1 사이로 맞춰주는 Normalization을 수행한다.

이를 통해 데이터들의 기준이 더욱 균일해지고 학습시에도 훨씬 더 빠르게 학습이 가능하다.

표준화 하는 대표적인 2가지 방법에는 **MinMaxscaler**와 **Standarization**이 있다.

<br />

### 1. MinMaxscaler

최대와 최소 값을 각각 1, 0으로 설정하여 계산하는 방법이다.

다른 값들은 최대보다 크지 않고, 최소보다 작지 않기에 모두 0과 1 사이에 위치하게 될 것이다.

$$ nx_{i} = \frac{x_{i} - min}{MAX-min}$$

예를 들어 x = [1, 101, 9, 2] 가 존재한다고 할 때

위 식에 따라 Max(x) = 101, min(x) = 1 인데, 각 값들을 위 계산식에 대입하면

$$ [\frac{1-1}{101-1} = 0, \frac{101-1}{101-1} = 1, \frac{9-1}{101-1} = 0.08, \frac{2-1}{101-1} = 0.01] $$

<br />
### 2. Standarization(표준화)

$$ X = \frac{X_{i} - μ}{σ}  (X ~ N(μ, σ^{2})) $$

기존에 고등학교 과정에 있는 표준화다.

Data Feature의 평균과 표준편차를 구한 후 `(feature 값 - 평균)/표준편차` 연산을 가한다.

이 결과 Feature의 분포는 표준정규분포 $$ N(0,1) $$를 따른다.

<br />
위 방식을 계산하는 것은 쉽고 간편하나, 만일 이상치들이 최대/최소 범위가 되는 경우에 계산이 비효율적일 수 있다.

이런 아웃라이어들의 영향을 최소화하기 위해서 [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) 같은 방식이 개발되었다.

<br />
<br />

## 2) Mini-Batch Gradient Descent

**Batch:** 우리가 가진 전체 Training data

$$ W · X + b = W · \begin{pmatrix}
 \vdots  & \vdots &    & \vdots \\
 X^{(1)} & X^{(2)} & \cdots & X^{(m)} \\
 \vdots  & \vdots &    & \vdots
 \end{pmatrix} + b $$

Gradient Descent까지 한 번을 학습할 때, Batch 하나(데이터 전체)를 사용하게 된다.

그런데 Colab이나 Jupyter에서 학습을 시킬 때, 한 바가 하나의 Training을 의미하진 않는다.

이는 인공지능 학습할 때 RAM이 이 많은 데이터를 한꺼번에 들 수 없기 때문에 쪼개서 넣는 것이다.

![minibatch](https://user-images.githubusercontent.com/86182583/130402762-ba1ed525-2681-43e4-ba60-c35c063bcb80.PNG)
<br />
원래 그림 속 데이터는 50000이지만, 정작 연산할 때 바 1개 당 1563개를 맡고 있다.

이는 계산해보면 50000 ÷ 1563 ≒ 32 란 소리다. 32등분 했다는 소리다. (이는 batch_size 세팅의 기본값)

이렇게 토막난 조각들을 **mini-batch** 라고 한다. Tensorflow에서는 batch_size를 정의하여 Training 모델을 정의한다.

모든 mini-batch가 학습이 완료될 때 Epoch 하나가 끝난다.

*참고로 완전히 나눠 떨어지지 않을 시 적더라도 남는 것끼리 Training을 한다*

<br />

### Mini-Batch Gradient Descent (용어)

1> **Batch Gradient Descent**: 한 번에 모든 Data를 Train한다.

2> **Mini-Batch Gradient Descent**: 한 번에 mini-batch씩만 Train한다.

3> **Stochastic Gradient Descent (SGD)**: 한 번에 1개 Data씩만 Train한다.

만일 SGD 이고 batch_size(등분)이 1 이상이면 한 번에 `batch_size`씩만 Train된다.

<br />

### batch_size에 따른 Gradient 관계

![stochbatdesc](https://user-images.githubusercontent.com/86182583/130405301-e6bdcd7d-be72-480f-87a8-b0969e65e84d.png)

클 수록 정확한 Gradient를 계산하는 경향이 있다. 작을 경우에는 빠르게 연산할 수 있지만,

그만큼 변화에 민감하게 반응하고 부정확해서 어느 정도의 Trade-Off를 갖고 있다.

ex) 선거 출구조사에서 표본 더 크게 뽑으면 정확해지지만, 비용과 통계 시간 때문에 전부 조사하지 않는 것과 비슷하다.

### batch_size 관련 Tip

1. 2의 거듭제곱으로 설정하는 것이 메모리에 효율적이다.

2. Batch-size는 클 수록 좋다. (메모리 초과 일어나지 않을 정도만)

3. 따라서 보통 32 이상으로 설정한다.

4. batch_size가 작을 수록, 어느 정도 Regularization으로 Overfitting을 방지하는 효과를 낼 수 있지만 그 효과가 적어 굳이 이것을 조정하진 않는다.

<br />
<br />

## 3) Optimizer

![goodlr](https://user-images.githubusercontent.com/86182583/130407968-fc4c4c08-692b-4530-b325-d24cbf090e44.png)

가장 이상적인 Learning Rate는 지속적으로 감소하는 형태가 나와야한다.

이런 형태로 만들기 위해서는 Initialization 설정도 있지만, Optimizer를 변경하여 조절할 수 있다.

<br />
### 1. **SGD** (Stochastic Gradient Descent)

Random하게 추출한 mini-batch씩 Gradient Descent를 조절한다.

다만 단점은 **해당 위치에서의 Gradient만 고려**하여, 만일 gradient가 위치별로 차이가 크면 minimum에 도달하기 어려울 수 있다.

또한 실제 최대/최소가 아닌 Local Min/Max에서 Gradient Descent가 멈출 수 있다.

우리에게는 전체적인 방향성을 고려하는 방법이 필요하다.

<br />

이런 부분에 머무르지 않도록 Gradient Descent에도 관성을 붙어주면 해결할 수 있다는 주장이 나왔다.

$$ W := W - \alpha\frac{𝝏}{𝝏W}cost(W) $$

따라서 Gradient(방향)나 Learning Rate(사이즈)에 관성을 주는 2가지 경우로 이론이 발전해나갔다.

![optimizerhistory](https://user-images.githubusercontent.com/86182583/130410765-4352673d-d398-43d4-bdcb-885925c9f313.png)

<br />
### 2. SGD + Momentum

$$ θ = θ (parameter) - v_{t} $$

$$ v_{t} = Γv_{t-1} + η(learning rate) · ▽_{θ}J(θ)(Gradient) $$

1> $$ v_{(t-1)} $$에 이전 방향이 저장되어 있다.

2> γ(momentum)를 곱해 이전 속도를 얼마나 반영할 지 결정

![mu1](https://user-images.githubusercontent.com/86182583/130418748-4747547c-0bdd-460b-89ae-73d5b425192a.PNG)

나온 방향대로 가는데 이전 관성의 영향을 받아 이동 경로가 달라진다.

![mu2](https://user-images.githubusercontent.com/86182583/130418487-98d08d36-fc60-4680-8f60-c4c10ee14d38.PNG)

벡터의 수직 방향 이동도 어느 정도 상쇄된다. (문제 1 해결)

또한 관성의 존재로 Local Minimum이나 안장점(기울기 0)도 빠져나올 수 있지만, 이것이 진짜 minimum도 빠져나올 시는 문제다.

γ 가 클 수록 더 잘 빠져나가지만, Brake도 잘 안 통하는 것과 같기 때문에 어느 정도만 커야 한다.

<br />
### 3. NAG (Nesterov Accelerated Gradient)

![nestrov_momentum](https://user-images.githubusercontent.com/86182583/130539479-731b3921-f7df-4fa7-bf67-99c65f628734.png)

$$ θ = θ - v_{t} $$

$$ v_{t} = Γv_{t-1} + η▽_{θ}J(θ - Γv_{t-1})$$

Momentum 방향으로 갔을 때 예상되는 지점의 gradient를 사용한다.

그래서 더 converge 잘 되고, minimum도 더 잘 찾는다.

<br />
이상 Gradient를 제어하여 조정하는 것이였고, Learning Rate를 제어하는 기법에 대해 알아보자

<br />
### 4. Adagrad (Adaptive Gradient)

$$ θ_{t+1} = θ - \frac{η}{\sqrt{G_{t} + ϵ(= e^{-8})}} · ▽_{θ}J(θ_{t})$$

$$ G_{t} = G_{t-1} + (▽_{θ}J(θ_{t}))^{2}$$

$$ G_{t} $$ 는 현재의 속력이고, $$ G_{t-1} $$ 에는 이전 속력이 저장되어 있다

**누적된 속력으로 Learning Rate를 나눈다.**

이를 통해 이전 속력$$(G_{t-1})$$이 계속 빨랐다면 Learning Rate로 나눠서 속력을 늦추고

이전 속력$$(G_{t-1})$$이 계속 느렸다면 Learning Rate로 나눠서 속력을 가속화한다.

다만 속력이 계속 증가 시 Learning Rate를 너무 큰 수로 나눠져서 학습이 안될 수 있다.

또한, 속력이 초반만 크고 나중에 작은 형태면 속력을 줄이지 않아도 되는데 줄어둔다.

이를 극복하기 위해 SGD에서 관성을 더하는 것과 같이 인접한 속력에 가중치를 주는 방식으로 극복한다.

<br />
### 5. Rmsprop (Root Means Squared Prop)

$$ θ_{t+1} = θ - \frac{η}{\sqrt{G_{t} + ϵ}} · ▽_{θ}J(θ_{t})$$

$$ G_{t} = ΓG_{t-1} + (1-Γ)(▽_{θ}J(θ_{t}))^{2}$$

$$ ϵ = e^{-8} , Γ = 0.9 $$ 를 주로 사용한다.

인접한 속력에 더 가중치를 가해서 Learning Rate를 나눈다.

이를 통해 이존 속력이 커서 Step Size가 0에 수렴하는 것을 방지한다.

여기서 **Gradient 와 Learning Rate에 둘 다 적용**시킬 수 있으면 좋을것이다.

<br />
### 6. Adam (Adaptive Moment Estimation)

Step Size와 Step Direction 모두에게 관성을 부여했다.

(방향) $$ m_{t} = β_{1}m_{t-1} + (1-β_{1})g_{t}$$

(속력) $$ v_{t} = β_{2}v_{t-1} + (1-β_{2})g_{t}^{2}$$

$$ m̂_{t} = \frac{m_{t}}{1-β_{1}^{t}} $$

$$ v̂_{t} = \frac{v_{t}}{1-β_{2}^{t}} $$

(통합) $$ θ_{t+1} = θ_{t} - \frac{η}{\sqrt{v̂_{t} + ϵ}}m̂_{t} $$

주로 $$β_{1} = 0.9   β_{2} = 0.999   ϵ = e^{-8}$$를 사용한다.

Adam이 대체적으로 개선된 성능을 보인다.

다만 학습이 진행될수록 Learning Rate가 작아져서 Bad Learning Rate로 수렴할 수 있는데,

이를 Rectify 하여 어느 정도 방지해주는 [Rectified Adam](https://arxiv.org/pdf/1908.03265.pdf)이 나왔다.

<br />

현재는 Adam이 가장 성능이 좋지만, 상황에 따라서 그렇지 않은 경우도 있기 때문에

여유가 있다면 가능한 전부 사용해서 그래프를 그려보고 가장 잘 되는 것을 사용하는 것이 좋다.

<br />
<br />
