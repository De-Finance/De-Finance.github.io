---
layout: post
title:  "[AI] 10. Saving Model, Callback"
subtitle: " Learn about finishing touches with the model"
category: [AI]
cover-img: /img/aipic.jpg
thumbnail-img: ""
tags: [AI]
comments: false
use_math: true
---
<br />

## 딥러닝 강의 10주차 - Saving Model, Callback

<br />

이번 글은 이론보단 실전 코드와 함께 정리를 해 놓았습니다.

<br />

# 1. The Method for saving model

모델을 저장하는 코드에서 .h5라는 파일이 저장되는데

이는 **HDF5**를 줄인 것이며, Hierarchical Data Format의 약자이다

기존에 과학자들이 데이터 관리할 때 주로 사용하였다.

공용 파일이기 때문에 모델을 저장하고 배포하기 용이하다.

<br />

## Model의 저장

h5 파일은 keras 모델의 다양한 요소를 저장하는데, 다음과 같은 요소를 포함한다.

- 모델의 레이어 및 연결방법을 지정하는 아키텍처 및 구성
- 가중치 값의 집합 (모델 상태)
- Optimizer
- 모델을 컴파일링하거나 `add_loss()` 또는 `add_metric()`을 호출하여 정의된 손실 및 메트릭의 집합

이런 요소들은 전체 혹은 일부로 저장할 수 있다.

하나는 train한 모델 전체를 저장하는 것이고

아니면 model의 architecture를 저장하고,

학습된 weight 값들만 저장하는 방법도 있다.

<br />

### 1. **모델 전체 저장**

저장을 할 때는 `model.save('파일이름.h5')` 로 **저장**한다. (저장 위치가 다르면 경로도 앞에 붙어준다.)

저장한 것을 불러올 때는 `new_model = load_model('파일이름.h5')` 이다.

모델 전체를 저장하면 바로 predict를 통해 학습된 모델을 사용할 수 있다.

np.argmax( `new_model.predict(x_test[0:9])`, axis=1)

<br />

### 2. **모델 architecture 저장**

JSON 파일 형태로 모델의 구성을 저장한다.

`json_string = model.to_json()` 로 모델의 구조를 저장한다.

`json_string`에는 다음과 같이 json 형태로 모델의 구조가 저장되어 있다.

```py
{"class_name": "Sequential", "config": {"name": "sequential", "layers": [{"class_name": "InputLayer", "config": {"batch_input_shape": [null, 784], "dtype": "float32", "sparse": false, "ragged": false, "name": "dense_input"}}, {"class_name": "Dense", "config": {"name": "dense", "trainable": true, "batch_input_shape": [null, 784], "dtype": "float32", "units": 32, "activation": "relu", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": null, "kernel_constraint": null, "bias_constraint": null}}, {"class_name": "Dense", "config": {"name": "dense_1", "trainable": true, "dtype": "float32", "units": 64, "activation": "relu", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": null, "kernel_constraint": null, "bias_constraint": null}}, {"class_name": "Dense", "config": {"name": "dense_2", "trainable": true, "dtype": "float32", "units": 128, "activation": "relu", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": null, "kernel_constraint": null, "bias_constraint": null}}, {"class_name": "Dense", "config": {"name": "dense_3", "trainable": true, "dtype": "float32", "units": 10, "activation": "softmax", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": null, "kernel_constraint": null, "bias_constraint": null}}]}, "keras_version": "2.4.0", "backend": "tensorflow"}
```

모델을 불러오는 것은 `model_from_json()` 함수를 사용한다.

```py
from tensorflow.keras.models import model_from_json
model_json = model_from_json(json_string)

model_json.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                 metrics=['accuracy'])

loss, acc = model_json.evaluate(x_test,  y_test, verbose=2)
```
이렇게 구조를 바로 불러오면 우리가 모델을 적지 않고 바로 컴파일 단계로 넘어갈 수 있다.

<br />

### 3. **모델 weight만 저장**

모델의 가중치 값만 저장되어 있는 형태다.

최근에 전이 학습에서 학습 시간을 획기적으로 줄이고자 다음과 같이 시작 가중치 값만 불러 새 모델을 train한다.

**저장** : `new_model.save_weights('파일이름.h5')`

**로딩** : `new_model.load_weights('파일이름.h5')`

<br />
<br />

# 2. Callback

학습을 하는 중간에 (epoch를 돌면서) 결과를 확인한다.

- ModelCheckpoint
- EarlyStopping
- LearningRateScheduler
- Tensorboard

등 모델을 만들 때 유용한 기능들에 대해 알아보자.

<br />

## 1) ModelCheckpoint

한 epoch의 끝인 특정 지점(checkpoint)을 저장하여 나중에 불러낼 수 있게 만든다.

[ModelCheckpoint 공식문서](https://keras.io/api/callbacks/model_checkpoint/)

<br />
### Checkpoint callback 만들기

```py
from tensorflow.keras.callbacks import ModelCheckpoint
import os

checkpoint_path = "training_1/cp.ckpt" # ckpt: checkpoint의 약자
checkpoint_dir = os.path.dirname(checkpoint_path)

# 체크포인트 콜백 만들기
cp_callback = ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)
```

<br />
### Checkpoint callback 사용하여 학습 시키기

```py
model = create_model()

<br />
model.fit(x,y, epochs = 2, validation_split = 1/6, callbacks = [cp_callback])
```

다음과 같이 fit model에 callback할 `.ckpt` 파일을 넣어준다.

하지만 다음 코드를 실행하면 epoch가 2여서 체크포인트가 2개가 나와야 하는데, 하나만 나온다.

이는 체크포인트 파일의 이름이 문제인데, 저장할 epoch마다 이름이 바뀌여야 한다.

```py
# 4자리 수 형식으로 epoch 값이 포함된 .ckpt 파일료 출력한다.
checkpoint_path = "training_2/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

cp_callback = ModelCheckpoint(checkpoint_path, verbose=1,
                              save_weights_only=True, period=1)
```

다음과 같이 저장하면 `cp-0001.ckpt` , `cp-0002.ckpt` , `cp-0003.ckpt` 같이 나온다.

다만 모든 포인트에서 저장하는 것이 아닌 일정한 학습을 *예) 1000번에 1번* 한 후 저장하고 싶을 때는 ModelCheckpoint 함수에 `save_freq =1000` 을 붙여주면 된다.

아니면 period를 조정하여 checkpoint라고 생각하는 곳의 간격을 늘린다.

<br />
### Checkpoint weight 불러오기

`load_weights('파일.ckpt')` 함수를 사용하면 불러올 수 있다.

**예시 코드**

```py
model = create_model()
model.load_weights('./training_3/cp-0003.ckpt')
loss, acc = model.evaluate(x_test,  y_test, verbose=2)
```

<br />

## 2) Early Stopping

```py
from tensorflow.keras.callbacks import EarlyStopping
```

![early-stopping-en](https://user-images.githubusercontent.com/86182583/133224211-1d5ae253-e814-482f-96ab-64a66acea726.png)

**Early Stopping 이란?** : 모델이 epoch를 돈 후 loss나 accuracy 같은 기준 값을 관찰하여, 만일 기울기가 실종되어 변화가 없거나,

값이 연속적으로 개선되지 않으면 학습을 중지한 후 값이 악화되기 전까지 학습한 모델을 최종 모델로 한다.

이 기능을 통해 무의미하고 비효율적인 학습을 없앨 수 있다.

[EarlyStopping 공식문서](https://keras.io/api/callbacks/early_stopping/)

- monitor: early stopping을 결정하는 기준
- min_delta: 이거보다 변화가 작으면 멈춘다.
- patience: 이 숫자의 epoch만큼 변하지 개선(감소)되지 않으면 멈춘다. (

**예시 :**

```py
es = EarlyStopping(monitor='val_loss', patience=2)
```
2로 설정시, 2연속 val_loss가 증가하면 train을 중지하고 모델을 완성한다.

![val_loss1](https://user-images.githubusercontent.com/86182583/133225281-9dc0b5e2-41fc-4730-b85a-1eb537f47661.PNG)

따라서 12 epoch에서 val_loss 값이 2연속 증가했으므로 학습을 중지했다.

![val_loss2](https://user-images.githubusercontent.com/86182583/133225471-63e7fafc-ac30-4d75-a42f-6e2721eb3cc9.PNG)

<br />

## 3) Learning Rate Scheduler

![decay_lr2](https://user-images.githubusercontent.com/86182583/133226381-db1b6274-cfba-4075-9e09-36ee8ecec295.png)

[Scheduler 공식문서](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules)

기존에는 함수에서 학습을 할 때 Learning Rate는 일정하게만 설정하였다.

하지만 초반에 값에 접근할 때는 더 크게 움직이면 결과에 영향을 주지 않고도 더욱 빠르게 학습할 수 있을 것이다.

이는 epoch에 따라 Learning Rate를 바꿔주는 schedule 함수를 만들어 어떻게 변할 것인지 설정하여 구현 가능하다.

```py
from tensorflow.keras.callbacks import LearningRateScheduler

def my_schedule(epoch, learning_rate=lr):
    if epoch < 5:
        return lr
    else:
        return float(lr * tf.math.exp(0.1 * (5- epoch)))
```

```py
lr_schedule_custom = LearningRateScheduler(my_schedule)
model = create_model()

model.fit(x, y,  epochs = 20, validation_split = 1/6, callbacks = [lr_schedule_custom], batch_size=512, shuffle=False)

```
<br />

### Tensorflow의 Schedular

위와 같은 학습을 Decay Learning이라고도 하는데, 이는 tensorflow로 더욱 쉽게 구현 가능하다.

```py
# Tensorflow의 Schedular 사용
from tensorflow.keras.optimizers.schedules import ExponentialDecay
lr_scheduler_exp = ExponentialDecay(lr, decay_steps=10000, decay_rate=0.96, staircase=False, name=None)

```

decay_steps 마다 decay_rate의 비율로 감소한다.

함수 형태로 표현하면 다음과 같다.

```py
def decayed_learning_rate(step):
    return initial_learning_rate * (decay_rate ** (step / decay_steps))
```

```py
model = exp_model()
sgd = SGD(learning_rate=lr_scheduler_exp, momentum=momentum, nesterov=False)
model.compile(optimizer=sgd, 
                  loss='categorical_crossentropy',
                 metrics=['accuracy'])
```


<br />

### ReduceLRonPlateau

해석하면 안정기, 즉 기울기 변화가 변화가 거의 없는 안정기에 들어갈 시, Learning Rate를 줄인다는 것이다.

```py
from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1)

model = create_model()

model.fit(x, y,  epochs = 20, validation_split = 1/6, callbacks = [lr_schedule_custom, reduce_lr], batch_size=512, shuffle=False)
```

![lrplateu](https://user-images.githubusercontent.com/86182583/133283474-69de7ef4-3238-42c9-b7e3-5e57310fdcf8.PNG)

설정된 조건에 도달했을 때 Learning Rate를 줄이는 것을 볼 수 있다.

<br />
<br />
