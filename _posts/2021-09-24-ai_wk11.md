---
layout: post
title:  "[AI] 11. Normalization Layers, Transfer Learning"
subtitle: " Learn about batch normalization and transfer learning"
category: [AI]
cover-img: /img/aipic.jpg
thumbnail-img: ""
tags: [AI]
comments: false
use_math: true
---
<br />

## 딥러닝 강의 11주차 - Normalization Layers & Transfer Learning

<br />
<br />

# 1. Normalization Layers

<br />

## Batch Normalization

![intcovshift](https://user-images.githubusercontent.com/86182583/134458659-fedf9303-0d8c-4d46-9bea-d6a238636822.PNG)

우리가 일반적으로 신경망 학습을 할 때는 학습을 할 수록 간단한 패턴에서, 더 구체적인 특징을 포착할 수 있도록 함수가 만들어진다.

배치 정규화 과정은 Train을 빠르게 하면서도 학습이 안정적일 수 있도록 만들기 위해 만들어졌다.

우리는 이전에 데이터셋이 있으면 학습하는 동안 기다리지 않고 Batch로 나눠서 병렬로 학습하는 것을 배웠다.

다만 이와 같은 학습 방법에도 문제가 생길 수 있다.

매번 각 노드에 임의의 데이터가 임의로 묶여서 들어가므로 분포가 달라진다.

이는 부정확한 학습을 하게 만든다.

![batch3](https://user-images.githubusercontent.com/86182583/134458467-32aaaf1f-a2eb-46ba-806e-5cd79f47287f.PNG)

이렇게 input domain이 다른 현상을 **Covariant Shift**라고 한다.

이를 일정하게 맞춰주는 것이 Normalization의 역할이다.

$$ Z^{(i)}_{norm} = \frac{Z^{(i) - 𝝁}}{\sqrt{σ^{2} + ε}} $$

따라서 이를 극복하기 위해서 어떤 batch가 들어가도 균일한 input이 될 수 있도록 설정해 준 것을 internal covariant shift라고 한다.

이를 통해 Gradient Vanishing 문제나, 가중치 초기화 문제를 해결할 수 있어서 dropout을 대체하여 많이 쓰이는 추세다.

다만, batch_size가 작으면 평균과 분산이 너무 부정확 해진다.

또한 이전 layer에서 weight 값을 참고하는 RNN 같은 경우 다른 feature들을 참고하여 받아서 학습방향이 매번 다를 것인데, 모든 layer에 Batch Normalize를 하기에는 학습이 너무 오래 걸린다.

<br />

## Layer Normalization

따라서 Batch Normalization의 단점을 극복한 **Layer Normalization**이란 기법이 개발되었다.

![bn_vs_ln](https://user-images.githubusercontent.com/86182583/134462360-fe089368-ae2e-44d8-9500-f55df34861dc.png)

위 그림을 들어 설명하자면 각 세로 줄이 하나의 sample(batch)이고, 같은 가로 줄은 하나의 특징(feature)이다.

Batch Normalization은 각 feature에 대한 평균과 분산을 구해서 batch에 있는 각 feature 를 정규화 하는 과정이였다.

반면 Layer Normalization은 각 input의 feature들에 대한 평균과 분산을 구해서 batch에 있는 각 input을 정규화한다.

기존에는 각 특징에 대해서만 계산하였지만, 이제는 층 단위로 평균과 분산을 맞추기 때문에 batch_size와 상관 없이 연산을 할 수 있다.

이를 계산하기 위해서는 𝝁와 σ를 구해서 Normalize해야 한다.

$$ net(estimate) = \frac{net-E(wx+b)}{σ}$$

x: N * D 인 Layer이면

𝝁, σ : N * 1

γ, β : 1 * D

$$ y = \frac{γ(x - 𝝁)}{σ + β}$$ 로 계산한다.

<br />
<br />

# Transfer Learning

전이 학습으로, 기존에 학습시키던 weight를 가져와서 활용하는 방식이다.

앞에서 말한 것처럼 신경망 같은 경우 우선 general한 feature를 포착한 다음, 학습을 해 갈 수록 더 구체적인 특징을 잡아간다.

하지만 처음에 initialization하는 것과 기초적인 특징을 잡는 과정들이 비슷한 과제가 있으면 여기서 미리 학습한 값을 활용해, 우리 문제에 맞도록 특화 시키면 된다.

AI 분야 같은 경우에는 Google, Microsoft, Naver 같은 IT 대기업들과 연구자들이 다양한 모델들에 관한 값들을 고성능 장비로 학습시키고 최적화된 값들을 제공하기 때문에 이런 Pretrainted된 모델을 갖다 쓰는 것이 경제적으로 시간적으로도 매우 도움이 된다.

완전히 같은 것은 아니기 때문에 일부 신경망은 freeze해야 하지만, 이 모델에 우리의 데이터를 학습해 주는 것으로 우리가 의도한 문제에 최적화된 신경망을 학습해 갈 수 있다.

특히 ImageNet 같은 모델이 대표적인데, 이미지 같은 경우에는 특히 학습시간이 오래 걸리고, 천만개가 넘는 데이터를 통해 학습된 모델이기 때문에 정확도를 올려주는데 탁월한 효과를 가지고 있다.

하지만 학습하는 이미지 과제는 다를 것이므로 초반의 feature extraction만 ImageNet filter를 사용하고 나머지는 우리가 학습시킨다.

나머지 우리의 데이터를 학습할 수 있도록 모델을 만들고 데이터로 추가 학습 시켜야 하는데, 이 과정을 **Fine Tuning**이라고 한다.

<br />

## Transfer Learning Strategy

데이터를 보유한 상황에 따라 효과적인 전략이 다르다.

![tlstrat](https://user-images.githubusercontent.com/86182583/134465675-03daee12-b0cf-4cd7-b5b4-15fd8cf2dcbc.PNG)

<br />

만일 학습시킬 데이터의 양이 적으면 오히려 왜곡이 일어날 수 있기 때문에 마지막 Fully Connected Layer만 학습 시키거나, 기초적인 High Level Feature만 학습시킨다.

만일 데이터가 많은 경우에는 Mid Level Feature까지 이용하여 학습을 한다.

데이터가 많을 수록 더 적은 layer를 freeze한 다음에 학습시킨다.

![tlst1](https://user-images.githubusercontent.com/86182583/134466946-d0c0f2cd-a31e-4bfc-9290-4fc5e798d2cd.PNG)

<br />

다만 overfitting이 일어나지 않도록 주의해야 해서 Learning Rate를 작게 한다.

또한 유사도도 살펴야 하는데, 만일 기존 작업과 매우 유사한 이미지를 다룰 때는 fine tuning을 크게 신경쓰지 않아도 되지만, 다를 경우에는 모델 학습에 우리가 갖고 있는 데이터가 중요하여 무조건 Fine-tuning 과정을 거쳐야 한다.

![tlst2](https://user-images.githubusercontent.com/86182583/134466983-b7bc072f-0ef0-4255-872e-da52533c2bc7.PNG)

<br />

[Learn more about transfer learning](https://www.tensorflow.org/tutorials/images/transfer_learning)

<br />
<br />
