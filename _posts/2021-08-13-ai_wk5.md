---
layout: post
title:  "[AI] 5. Holdout Validation & Model Fitting"
subtitle: " Learn about Data Split & Model Fitting "
category: [AI]
cover-img: /img/aipic.jpg
thumbnail-img: ""
tags: [AI]
comments: false
use_math: true
---

## 미래연구소 16기 강의 기반 정리

[http://futurelab.creatorlink.net/](http://futurelab.creatorlink.net/)

<br />

## 딥러닝 강의 5주차 - Holdout Validation & Model Fitting

<br />

본격적으로 모델을 학습할 때 필요한 딥러닝 기법을 학습할 것이다.

<br />

## 0) Hyperparameter

Parameter: (W, b) 같이 모델 내부에서 결정되는 변수

Training을 하면서 Model의 Data를 통해 자동으로 변동된다.

Hyperparameter: 사용자가 모델을 설정할 때 **직접 설정**해주는 값이다.

이를 조정하여 최적의 값을 구한다.

- Learning Rate
- Layer 개수
- Layer 내 Node의 개수
-  Iteration (Epoch) - (Gradient Descent를 몇 번이나 할 것인지)

<br />

## 1) Holdout Validation

딥러닝 모델을 만들기 위해서는 학습할 데이터를 입력으로 넣어야 한다.

하지만 정작 학습(Training)을 할 때는 모든 데이터를 입력으로 사용하지 않고 일부를 남기게 된다.

그 이유는 이 분리된 일부를 학습이 적절하게 되었는지 Test Case로 사용할 것이기 때문이다.

이처럼 검증을 위해 초반에 데이터셋을 분리하는 것을 *Holdout*이라고 한다.

<br>

만일 Test하는 데이터셋이 없으면 어떤 일이 발생할까?

이는 마치 시험을 공부하는데 문제집 하나만 계속 반복해서 푸는 것에 비유할 수 있다.

만일 같은 문제가 출제되면 성공적으로 풀 수 있겠지만, 유형이 바뀌면 풀지 못할 수도 있다.

인공지능 모델이 실생활에서 유용하려면 General한 상황에서도 맞게 예측할 수 있어야한다.

따라서 일부를 나눠서 지금까지 본 적 없는 상황에서도 대응이 가능한지 확인하도록 하는 것이다.

<br />

Dataset은 보통 **Training Set, Validation Set, Test Set** 으로 나뉜다.

![trvate](https://user-images.githubusercontent.com/86182583/128814225-dbe5f7e2-e0d1-4f62-86fb-41e56485df88.png)

## Dataset의 종류
<br />
## 1. Training Set

모델을 학습시키는 데이터다. 최적의 Parameter를 찾기 위해 Gradient Descent를 하면서 parameter를 업데이트된다.

(보통 학습을 하는 부분이 가장 중요하기에 비중을 높게 책정한다)

<br />
## 2. Validation Set

내가 설게한 모델(Custom Hyperparameter를 통해 만든 모델) 최적의 모델인지 검증하는 Dataset이다.

(그 동안 학습한 방향이 다른 문제들에서도 활용 가능한지 알아보는 **모의고사**의 역할을 한다)

Validation Set를 통과시킨 후 결과를 보고 모델이 최적인지 검정한다.

만일 부족한 점이 있다면 Hyperparameter를 직접 조절하면서 가장 효과적인 모델을 찾아내야한다

**모의고사를 외우는 것**은 향후 General Problem의 대체 능력을 측정할 수 없게 되므로

이 데이터는 Gradient Descent에 반영하지 않습니다.

<br />
## 3. Test Set

여러번의 Validation을 거친 모델을 **최종 Test**할 때 사용되는 Data이다.

(즉, 모의 평가를 통해 만들어진 실력을 **수능**에서 사용하는 것과 같다)

이것은 실전이며 **낮선** 데이터를 통해 성능을 평가하는 것이 목적이라 단 한번만 행해진다.

따라서 tuning이나 gradient descent 같은 절차는 없고, 한 번만 evaluate 해서 결과를 내는 것으로 끝난다.

<br />

### Training, Validation, Test 비율은 어떻게 정해야할까?

이는 각 Dataset마다 최적의 구조가 다르므로, 비슷한 데이터에 대한 경험적인 지식을 참고하거나

직접 적절한 Hyperparameter 조정을 통해 찾아다닐 수 밖에 없다 (**No Free Lunch Theorems**)

<br />

## Holdout 2가지 방식
<br />
### 1) 2-way Holdout = (Training + Test)

여기서 Test는 사실상 Validation의 역할을 하고, 최종 테스트는 존재하지 않는다.
<br />
### 1) 3-way Holdout = (Training + Validation + Test)

Training Data로 학습하고, Validation으로 검증하는 수차례 과정을 거친 후

최적의 모델을 찾았을 때 마지막으로 Test Set으로 검증해준다.

딥러닝 모델을 만들 때 가장 보편적인 방식이다.

<br />
<br />
## 2. Model Fitting

**Fit** 은 Train의 또 다른 이름이기도 하며(주로 CS에서 사용)

Training set 만이 아닌 Test Set에서도 예측을 잘하는 모델을 지칭한다.

우리는 모델을 학습할 때는 데이터의 일부를 (Train set, Validation Set, Test Set)으로 나누고

Epoch를 여러번 돌려서 반복 학습시킨다.

다만 여기서 문제는 모델을 어느 정도까지 학습해야하는지에 관한 문제가 존재한다.

![underfitting_overfitting_001](https://user-images.githubusercontent.com/86182583/129317594-2c7d04cf-d61b-42ca-a8ce-c433dc4c23ca.png)

<br>
### 1> Underfit (1번 그림)

마치 문제집을 덜 풀은 상태에서 시험을 보는 것과 같은 상태다.

따라서 처음에 잡은 값은 실제 값으로 Gradient Descent로 덜 이동했기에,

예측을 잘 못하는 상태다.

비선형성을 더하는 activation도 덜 되었기 때문에 가벼운 모델의 경우

선형에 가깝다.

<br>

### 2> Fit (2번 그림)

 Train할 수록 주어진 데이터에 접근하기 때문에 예측률이 좋다.

 다만 이것이 unseen data에 관해서도 맞을 지는 모르기 때문에 Validation Set과 Test Set을 통해 결과를 확인한다.

 학습을 이 때 중단할 수 있으면 가장 좋다.

<br>
### 3> Overfit (3번 그림)

같은 종류의 문제집만 너무 많이 봐서 거의 답을 외운것 같은 상태이다.

만일 그 문제집과 똑같은 문제가 나오면 정확도가 매우 높겠지만,

새로운 유형(Unseen data) 같은 경우 오히려 대응력이 떨어질 수 있다.

<br>

![optimal_model](https://user-images.githubusercontent.com/86182583/129318774-1b7e43fb-f9be-4e05-8d0e-d2463d6fc4f9.png)

따라서 Overfit이 나기 전인 지점에서 멈추면 최적의 모델을 찾아낼 수 있다.

<br>

여기서 가장 Fit한 상태를 찾기 위해서 참고하는 값은 상황에 따라 다르지만,

Test(Test/Validation Set)하는 데이터셋의 Loss값이 감소하다가 다시 증가하거나

Accuracy가 상승하다가 다시 떨어지는 점을 보통 기준으로 삼는다.

이 점을 bias-variance trade-off라고 소위 칭하는데,

<br>

**Bias(편향)**: 참 값들과 추정 값들의 차이 (error). 데이터의 불충분으로 오차가 크면 관계가 안 나타나는 Underfitting이 발생한다.

**Variance(분산)**: 추정값들이 흩어진 정도. 분산값이 높으면 너무 많은 것을 설명하려 들어 오차가 발생하는 Overfitting이 발생한다.

<br>

결국 이 두가지를 작게 만드는 것이 필요하다.

다만 한 가지를 잡으면 나머지 하나는 포기해야하기 때문에 그 중간점을 적절히 잡는 것이고

이를 **Bias-Variance Trade-off** 라고 부른다.

<br>
![lbhv](https://user-images.githubusercontent.com/86182583/129323394-38570914-b7d2-406f-aa1b-e99563494e12.png)

<br>
예시를 통해 이해를 해 보자

### 예시 1

![underfitting_overfitting_002](https://user-images.githubusercontent.com/86182583/129322046-0b7c4ba4-edee-44fe-9493-0917dde87120.png)

| 높은가? 낮은가? | 1번 | 2번 | 3번 |
| --- | --- | --- | ---|
| Training Error | ↑ | ↓ | ↓ |
| Dev(Val) Error | ↑ | ↓ | ↑ |

**1번:** 실제 구분에 비해 데이터가 너무 학습이 안 되어 그 어떤 것도 맞지 않는 Underfitting 상태다.

**2번:** 적절하게 학습한 Fit 한 상태다.

**3번:** Training Data와는 딱 맞지만, Dev처럼 학습 안한 모델에서는 오히려 에러가 다시 증가한 Overfitting 상태다.
<br>
### 예시 2

각각이 어떤 상황인지 나타내서 Overfit인지 Underfit인지 알아보자

| 높은가? 낮은가? | 1 | 2 | 3 | 4 |
| --- | --- | --- | ---| --- |
| Training Error | 1% | 15% | 15% | 0.5% |
| Dev(Val) Error | 11% | 16% | 30% | 1% |

1) Low Bias & High Variance (Overfit)

2) High Bias & Low Variance (Underfit)

3) High Bias & High Variance (Underfit)

4) Low Bias & Low Variance (Optimal Fit)

다만 위에 예시에 나타난 것 처럼 구분값이 절대적인 것이 아니라 상대적으로 이해해야 한다.
<br>
### 예시 2-2

|     | 1 | 2 | 3 | 4 |
| --- | --- | --- | ---| --- |
| Optimal Accuracy | 99% | 99% | 99% | 99% |
| Training Set Accuracy | 80% | 98% | 95% | 60% |
| Validation Set Accuracy | 75% | 97% | 70% | 50% |

아래를 드래그
<br>
<span style="color:white">|1: High Bias & Low Variance |2: Low Bias & Low Variance |3: Low Bias & High Variance |4: High Bias & High Variance |</span>
