---
layout: post
title:  "[AI] 4. Activation & Initialization"
date:   2021-08-03
excerpt: " Detailed Explanation about Activation function & Initialization "
category: [AI]
cover-img: /img/aipic.jpg
tag:
- AI
comments: false
use_math: true
---

## 미래연구소 16기 강의 기반 정리

[http://futurelab.creatorlink.net/](http://futurelab.creatorlink.net/)

<br />

## 딥러닝 강의 4주차 - Activation & Initialization

<br />

딥러닝은 일종의 거대하고 복잡한 함수를 만드는 과정이다.

이를 통해 Regression 문제나 Classification 문제를 해결한다.

Linear Regression 같은 경우 일차함수로 비교적 구하기 간단하다

하지만 고양이 분류 같이 이미지 같은 고차원 데이터를 input으로 넣는 경우는

모델이 간단하게 나오지 않는다.

이런 복잡한 모델을 만들기 위해서는 함수 형태가 non-linear해야 한다.

이를 위해 Layer들의 Activation function을 거치며 비선형성을 더해간다.

따라서 모델을 거치면 딥러닝이라 불리 거대한 합성 함수가 나온다.

*(단, Linear 같은 일차함수는 연속적으로 거쳐도 일차함수 꼴만 나오므로 주의)*

<br />

## 1) Activation Function

앞서 사용된 sigmoid 함수 처럼, 신경망에서 input과 weight를 더해준 후,

결과 값에 대한 출력을 어떤 방식으로 할 지 정해주는 함수이다.

<br />

## Activation Function의 종류
<br />
## 1. Sigmoid

![sigmoid4](https://user-images.githubusercontent.com/86182583/127945334-0061e0e2-c454-4970-9a1c-0cd623576992.png)

$$ g'(z) = a(1-a) $$

**1) 0 < g(z) < 1**

장점: binary classification의 결과를 내는 output layer 라는 특수한 상황에 적합하다.

**2) g'(z)가 0에 가까운 구간이 많다.**

단점: Gradient Descent의 속도가 저하된다.

<br />
## 2. tanh (hyperbolic tangent)

![tanh](https://user-images.githubusercontent.com/86182583/127978905-b35b8fc4-c447-4651-912b-339a1c7c9b26.png)

$$ g(z) = -\frac{e^z - e^{-z}}{e^z + e^{-z}} $$

$$ g'(z) = (1-a^2) $$

**1) -1 < g(z) < 1**


**2) g'(z)가 0에 가까운 구간이 많다. (0 < g'(z) < 1)**

장점: sigmoid보다 vanishing gradient가 덜하다. (g'(z)의 최대값이 1)

단점: Gradient Descent의 속도가 저하된다.

<br />
## 3. ReLU (Rectified Linear Unit)

![relu](https://user-images.githubusercontent.com/86182583/127980370-57dd78f4-b1df-47fb-9dfd-d680c437da1d.png)

$$ g(z) = \begin{cases} z & z>0 \\ 0 & z<0 \end{cases} $$

$$ g'(z) = \begin{cases} 1 & z>0 \\ 0 & z<0 \end{cases} $$

**1) g'(z) = 1인 구간이 절반이다. (0 < g'(z) < 1)**

장점: sigmoid, tanh의 vanishing gradient 문제 해결

단점: 그래도 절반의 gradient가 0이다. (dying ReLU)

<br />
## 4. Leaky ReLU

![leakyrelu](https://user-images.githubusercontent.com/86182583/127982439-fd1bb5b5-10d6-40fd-83c0-1e8021d7d673.png)

$$ g(z) = \begin{cases} z & z>0 \\ 0.01z & z<0 \end{cases} $$

$$ g'(z) = \begin{cases} 1 & z>0 \\ 0.01 & z<0 \end{cases} $$

**1) g'(z) = 0인 구간이 없다**

장점: 기존 dying ReLU 현상을 해결 (GAN과 같은 train이 어려운 경우에 사용)

<br />
## Activation Function 정리

![activef](https://user-images.githubusercontent.com/86182583/127980107-097799fa-8926-4fd6-be2e-dddf8693c9e4.png)

<br />
## Output Layer의 Activation Function의

![outputactive](https://user-images.githubusercontent.com/86182583/127983230-a15f590e-24d1-438b-9018-68cd0f2bb901.PNG)

<br />
<br />
<br />

# 2) Random Initialization

우리가 처음에 input weight의 시작점을 잡는 것을 Initialization 이라 한다.

이전 글 예시에서는 weight matrix를 0으로 초기화하여 계산하였다.

$$ W^{[l]} = \begin{pmatrix}0 & 0\\\ 0 & 0\end{pmatrix} $$

다만 이 방법은 실제로 문제를 푸는데 적합하지 않다.

### 1. Row symmetric

$$ W^{[l]} $$ 가 모두 0이면, $$ g(W^{[l]} X + b) = A^{[l]} $$ 행들의 원소가 모두 같아진다.

이를 **Row symmetric** 하다고 한다.

(b 값을 제어하면 symmetric을 깨는 것은 가능하지만, Row symmetric은 안 깨진다)

결국 아무리 node를 여러개로 설정해도 하나 사용하는 것과 같아 효용이 없다

<br />

### 2. Vanishing Gradient (기울기 소실)

$$ W^{[l]} = W^{[l]} - \alpha dW^{[l]} $$

$$ dW^{[l]} = \frac{1}{m}dZ^{[l]} A^{[l-1]T} $$

Activation function의 접선의 기울기(미분 계수)가 0에 가까워지는 현상이다.

Gradient Descent를 할 때 기울기가 소실되면 아무리 Gradient Descent를 반복해도 계산이 안될 것이다.

즉, 학습이 잘 일어나지 않는 것이다.

기울기는 $$ dW^{[l]} $$ 가 좌우하는데, 이는 $$ W^{[l]} $$ 의 미분값이다.

따라서 이 값을 0으로 초기화한다는 것은 Vanishing Gradient 문제를 야기한다.

### 용어 정리

같은 현상인데 설명하는 관점에 따라 다르다.

**1> saturation:** weight의 update가 잘 일어나지 않는 현상

**2> vanishing gradient:** gradient가 0에 가까워지는 현상

<br />

## 해결

### 1. np.random.randn

$$ w^{[l]} = np.random.randn(shape) $$

Weight에 Random한 값을 부여해 W[l]이 row symmetric 되는 것을 막는다

### 2. 0이 아닌 적절한 상수를 곱한다.

![sigmoid4](https://user-images.githubusercontent.com/86182583/127945334-0061e0e2-c454-4970-9a1c-0cd623576992.png)

1> 우선 0보다 큰 수를 곱한다 (-면 방향이 바뀔 수 있음)

2> W가 너무 크면 빠르게 1이나 0으로 수렴하여 업데이트가 없어지는 saturation 현상이 발생할 수 있다.

따라서 앞에 작은 수를 곱해준다.

## 결론 : Small Random Numbers

$$ W^{[l]} = 0.01 * np.random.randn(shape) $$

N(0,1) 인 가우시안 분포에서 임의에 작은 값을 뽑아 배포한다.

<br />

다른 초기화 방법들도 알아보자

## +) Xavier Initialization

실제 딥러닝에서 널 사용되는 방식

모든 Layer에 대해 분산이 같도록 맞춰준다.

Input과 Output Node의 개수를 고려하는 방법이다.

이 방법은 tanh 나 sigmoid에 관해서 잘 작동한다.

W = np.random.randn($$ n^{[l-1]}  $$, $$ n^{[l]}  $$) / np.sqrt($$ n^{[l-1]} $$)

![xvi1](https://user-images.githubusercontent.com/86182583/127959917-57bb86b0-51aa-4fdd-ab1f-08a41871a19e.PNG)

정규 분포일 때

<br />

![xvi2](https://user-images.githubusercontent.com/86182583/127959950-1493d1c2-550c-4a09-834a-b9ad7a844290.PNG)

Uniform 분포일 때

## +) He Initialization

Xavier Initialization의 변형이다.

ReLU와 Xavier를 함께 사용시 weight 분포가 0이 되버린다.

이때는 **He Initialization**을 한다.

![He1](https://user-images.githubusercontent.com/86182583/127960706-e3678adc-02b5-4354-8f65-fc61c39d0533.PNG)

![He2](https://user-images.githubusercontent.com/86182583/127960760-fc3a7140-703a-49e5-b87f-71bc421e9b6d.PNG)

<br />

이 외에 가장 많이 하는 Initialization은 Pretrained 된 것을 가져오는 것이다.

처음부터 학습하기에는 시행착오 과정이 너무나 많이 소요될 수 있으므로 이미 연구되고

검증된 data를 통해 더 빠르고 편하게 학습을 시작할 수 있다.

<br />
