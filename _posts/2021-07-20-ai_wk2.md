---
layout: post
title:  "[AI] 2. Regression"
subtitle: "More Detail in Supervised Learning"
category: [AI]
cover-img: /img/aipic.jpg
thumbnail-img: ""
tags: [AI]
comments: false
use_math: true
---

## 미래연구소 16기 강의 기반 정리

[http://futurelab.creatorlink.net/](http://futurelab.creatorlink.net/)

## 딥러닝 강의 2주차 - Regression & Logistic Regression

<br />
<br />

# 1) Regression (회귀)

![선형회귀모델](https://user-images.githubusercontent.com/86182583/125404648-cf54d980-e3f1-11eb-9c65-e3face6be066.PNG)

최적의 함수에 대한 기준

데이터(x)와 결과 값(y)이 있다면 이 결과를 가장 잘 설명해 줄 모델을 찾아간

__Data → Prediction (가정) → 최적의 모델__

이렇게 찾아가는 방식을 회귀라고 한다

### 설명
모델이 선형(Linear)일 때를 가정한다

#### H(x) = Wx + b

데이터들은 2차원 좌표계에서 존재하는 하나의 점이다

이들의 패턴을 가장 잘 설명할 수 있는 모델을 찾는 것이 목표다

#### **어떤 모델이 점이 가장 이상적인 모델인가?**

<br />

존재하는 모든 점들과의 거리가 가장 가까워야한다

이를 수학적으로 설명하면 Cost Function이 나온다

![cost_function](https://user-images.githubusercontent.com/86182583/125410808-3bd2d700-e3f8-11eb-91f7-874b9d4c15ee.PNG)

#### **Cost가 가장 작을 수록 모델을 가장 잘 설명할 수 있는 모델이다**

이는 다음과 같이 예측한 모델에다 수선의 발을 내린 후

그 거리^2 (음수를 고려한제곱)의 의 평균을 구하여 Cost를 계산한다

이것이 소위 오차를 계산하는 손실함수 중 하나로 사용된다 : MSE (Mean Square Error)

#### **어떻게 가장 최적인 부분을 찾는가?**

W(기울기)와 b(y 절편)값을 조정해가면서 찾는다

변화율 등을 고려하여 에러가 가장 최소화 되는 방향을 찾는다

이 w와 b를 조정하여 cost가 최소로 나오는 것이 **최적의 function**이다.

#### **최소 Cost를 구하는 방법은 Gradient Descent를 사용한다**

![cost_function2](https://user-images.githubusercontent.com/86182583/126597644-2a7a510c-f9a8-4ad1-baad-b1fd5a912c87.PNG)

1. 시작점을 결정하고 (Initialization)

2. (미분을 통해)접선의 기울기를 구하며

3. Learning Rate만큼 접선의 방향으로 w를 이동시킨다. (업데이트)

<br />
<br />

***

# 2) Logistic Regression (로지스틱 회귀)

>> Regression (값 예측 문제) != Logistic Regression (분류 문제)

Linear Regression에서는 함수의 범위는 없으며, 실제 값 차이인 Cost를 줄이는 것이 주 관심사였다.

다만 Classification에서는 특정 상태로 분류하려면 범위가 필요하다.

따라서 Log 함수를 이용한 Logistic Regression이 나오게 되었다.
<br />

ex) 고양이 이미지를 넣으면 그것이 고양이가 맞는지 아닌지 판단한다

<br />
<br />

## 0 : Dataset & Preprocessing

데이터들을 수집하고 전처리 할 것

컴퓨터는 사람처럼 이미지를 보고 판단할 수 없다

전부 숫자의 배열 형태로 바꿔줘야하는데, 이렇게 해서 나온 배열형 좌표들을 벡터화라고 한다

### 벡터화 (Vectorization)

$$ x^{(m)} = \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \\ \vdots \\ x_{n_{x}} \end{pmatrix} $$

한 행은 데이터를 이루는 feature들이 n개 존재하고, 데이터는 m개 존재하여 x를 이룬다

+) 컬러 이미지의 경우 각 픽셀마다 RGB의 값이 있는데, 이들을 뽑아주고 1차원 matrix로 붙어준다

이를 Flatten이라 하는데 이렇게 만드는 이유는 2차원 이상 좌표를 연산할 때 반복문을 사용하면 앞 연산이 끝날 때까지 대기해야하지만

1차원으로 만들면 프로세서들이 병렬로 계산할 수 있게 만들어 주어 연산속도를 높힌다.


<br />


## 1 : Initialization

x와 y는 이미 데이터가 주어졌으므로,

각 feature(x)에 적용되는 weight(가중치)인 **w**와

편향(bias)인 **b** 값을 임의로 설정해주는 과정이다

<br />

## 2 : Forward Propagation

x, w, b 를 통해 우리가 예측하는 결과 y를 계산한다

실제 y와는 오차가 있을 수 있으므로 ŷ 으로 표현한다

우선 선형 회귀와 마찬가지로 다음과 같이 계산하면 된다.

$$ ŷ = w_{1} x_{1} + w_{2} x_{2} + w_{3} x_{3} + \dots + w_{n_{x}} x_{n_{x}} + b $$

$$ = \begin{pmatrix} w_{1} & w_{2} & w_{3} & \dots & w_{n_{x}}\end{pmatrix}  \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \\ \vdots \\ x_{n_{x}} \end{pmatrix} + b$$

$$ = w^T x + b $$


여기서 끝나지 않고 앞서 말한 분류에 적합한 데이터를 만들기 위해 sigmoid 함수를 적용해준다.

이 함수는 0을 기준으로 무한으로 뻗어갈 수로 1에 수렴하고,

반대로 음수로 갈 시 0으로 수렴하여 ŷ의 범위를 0과 1사이 위치시킨다.

따라서 분류 기준만 잡으면 Binary Classification을 하기에 적합하다.
<br />

![sigmoid](https://user-images.githubusercontent.com/86182583/126854035-11606079-128a-45aa-9d57-d1f8848ef91e.png){: .mx-auto.d-block :}
<br />

예측 값 최종 계산

$$ a = \partial(w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}$$

<br />

## 3 : Compute Cost

마찬가지로 실제값과의 차이가 작아야 더 정확한 모델이다.

계산을 한 후 **각 데이터에 대한 Loss Function**을 적용하고,

이들을 평균내어 **cost** 함수를 계산한다.

기존 Linear Regression에서는 MSE를 오차 계산하는데 사용한 것 처럼

Binary classification에 가장 적합한 것은 **Cross Entropy Function이다**

### Loss 함수

실제 값과 예측 값을 넣고 계산을 한다

$$ Loss(a,y) = \partial(ŷ) = \partial(w^T x+b), y) = -(y log(a) + (1-y) log(1-a))$$

$$ Cost(w,b) = \frac{1}{n} \sum_{i=1}^N Loss(y_i, \partial(w_i+b)) $$

<br />

## 4 : Back Propagation

Gradient Descent Parameter를 업데이트할 방향을 구한다.

이는 즉 접선의 기울기를 통해 구한다.

지금까지 계산했던 과정을 역으로 돌아가는데, $$ \frac{\partial J}{\partial w}, \frac{\partial J}{\partial b} $$

를 구하는 것이 목적이다.

![backp](https://user-images.githubusercontent.com/86182583/126855072-f3068921-39b7-4d54-ad51-88b46ba8fad6.PNG)

예를 들어 $$ L(a, y) = -(y log(a) + (1-y) log(1-a)) $$ 였는데, $$ \frac{dL}{da} $$는 a에 대해 미분해야한다.

따라서, 결과가 $$ -\frac{y}{a} + \frac{1-y}{1-a} $$ 로 나온다

이런 식으로 계속 미분해가면, $$ \frac{da}{dz} = a - y $$

$$ \frac{dz}{dw_1} = x_1 $$ 처럼 우리가 원하는 값을 계산할 수 있다.

<br />

## 5 : Gradient Descent

4 단계에서 구한 $$ \frac{\partial J}{aw}, \frac{\partial J}{ab} $$

cost를 줄이는 방향으로 Learning rate만큼 이동한다

$$ w ⇐ w - α ⋅ \frac{\partial J}{\partial w} $$

$$ b ⇐ b - α ⋅ \frac{\partial J}{\partial b} $$

다음 과정들을 반복하여 최적에 도달한다.
